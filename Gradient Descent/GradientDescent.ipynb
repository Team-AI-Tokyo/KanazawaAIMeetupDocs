{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAIM 2017\n",
    "\n",
    "### Gradient Descent Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from scipy.special import gamma\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Custom Function\n",
    "\n",
    "In this case, we have chosen a fairly simple convex function - A product of two $2D$ distributions namely  Caucny distribution and the Gaussian. This simple example illustrates how optimization is done on probabilistic models wherein the objective function is almost always a product of distributions.\n",
    "\n",
    "Another interesting aspect of this construct is that the following optimization if infact a *Bayesian Inference* with a sparse prior (Cauchy in this case).\n",
    "\n",
    "$$\n",
    "J = \\mathcal{C}(\\mathbf{x})\\cdot \\mathcal{N}(\\mathbf{x}) = \\frac{1}{1 + \\frac{\\mathbf{x}^2}{\\nu}} \\cdot \\exp \\bigg [-\\frac{(\\mathbf{x} - \\mu)^2}{2\\boldsymbol  \\Sigma} \\bigg]\n",
    "$$\n",
    "\n",
    "The above distribution is unnormalised for convenience. The gradient is given by\n",
    "\n",
    "$$\n",
    "\\nabla J = 0.5 \\Sigma^{-1} \\mathbf{x}^T  +  \\bigg (\\frac{1}{1 + \\frac{\\mathbf{x}^2}{\\nu}} \\bigg )^2 \\frac{\\mathbf{x}}{\\nu}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribution Parameters\n",
    "v = 20\n",
    "mu_x = 0\n",
    "sigma_x = np.sqrt(19)\n",
    "\n",
    "mu_y = 1\n",
    "sigma_y = np.sqrt(8)\n",
    "mu = np.array([mu_x, mu_y])\n",
    "sigma = np.array([[sigma_x, 0],[0,sigma_y]])\n",
    "\n",
    "def Cauchy(x,y):\n",
    "    cauchy = 1. / (1. + (x) ** 2 * (y) ** 2 / v)\n",
    "    return cauchy\n",
    "\n",
    "def Gauss(x,y):\n",
    "    gauss = np.exp(-0.5 * (x - mu_x) ** 2 / sigma_x ** 2 -\n",
    "                   0.5 * (y - mu_y) ** 2 / sigma_y ** 2)\n",
    "    return gauss\n",
    "\n",
    "def Comp_Dist(x,y):\n",
    "    return -Gauss(x,y)*Cauchy(x,y)\n",
    "\n",
    "def Grad_J(x):\n",
    "    grad = 0.5*np.dot(np.linalg.inv(sigma),(x - mu).T) \\\n",
    "               + Cauchy(x[0],x[1])*2*x/v\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting Function\n",
    "def Plot(x,y,J, grad = None, orth = False):\n",
    "    plt.figure(figsize=(12,7))\n",
    "    ax = plt.subplot(111)\n",
    "    plt.contourf(x,y,J,cmap='inferno')\n",
    "    \n",
    "    if grad is not None:\n",
    "        plt.plot(grad_x[:,0],grad_x[:,1], '-', color = '#217C7E', marker = '.',\n",
    "         markersize= 15, label = \"Gradient Descent\")\n",
    "        legend = plt.legend(frameon=True,fontsize= 13)\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    plt.xlabel('$x$',fontsize= 16)\n",
    "    plt.ylabel('$y$',fontsize = 16)\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    empty_string_labels = ['']*len(labels)\n",
    "    ax.set_yticklabels(empty_string_labels)\n",
    "    ax.set_xticklabels(empty_string_labels)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if orth == True:\n",
    "        plt.ioff()\n",
    "        fig = plt.figure(figsize=(12,7))\n",
    "        ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "        surf = ax.plot_surface(x,y,J, cmap = 'inferno', linewidth=1, antialiased=True, shade=False)\n",
    "        plt.tight_layout()\n",
    "        ax.view_init(44, -49)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = np.mgrid[-10:10:0.08, -10:10:0.08]\n",
    "\n",
    "gauss = Gauss(x,y)\n",
    "cauchy = Cauchy(x,y)\n",
    "# Compound Distribution\n",
    "J = Comp_Dist(x,y)\n",
    "\n",
    "# Plot the 2D compound distribution\n",
    "Plot(x,y,J, orth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 epochs, Cost = -0.1970237948545262, Gradient Step = [ 0.2941362   0.56077984]\n",
      "After 2 epochs, Cost = -0.3375015328241495, Gradient Step = [ 0.27734029  0.49065704]\n",
      "After 3 epochs, Cost = -0.5124192037115258, Gradient Step = [ 0.26259082  0.43277182]\n",
      "After 4 epochs, Cost = -0.6867767579864288, Gradient Step = [ 0.2453379   0.37881605]\n",
      "After 5 epochs, Cost = -0.8226991340377451, Gradient Step = [ 0.2215892  0.3219697]\n",
      "After 6 epochs, Cost = -0.9080043696734255, Gradient Step = [ 0.19162311  0.26214993]\n",
      "After 7 epochs, Cost = -0.9540914417787556, Gradient Step = [ 0.1597466   0.20518857]\n",
      "After 8 epochs, Cost = -0.9768836181796746, Gradient Step = [ 0.13015484  0.1563739 ]\n",
      "After 9 epochs, Cost = -0.9875088202397726, Gradient Step = [ 0.10478029  0.11737724]\n",
      "After 10 epochs, Cost = -0.9921404097798228, Gradient Step = [ 0.08386544  0.08740013]\n",
      "After 11 epochs, Cost = -0.9939140515871898, Gradient Step = [ 0.06694117  0.06480436]\n",
      "After 12 epochs, Cost = -0.9943718553126978, Gradient Step = [ 0.05336172  0.04794084]\n",
      "After 13 epochs, Cost = -0.9942613345045542, Gradient Step = [ 0.04250912  0.03542015]\n",
      "After 14 epochs, Cost = -0.9939390031444116, Gradient Step = [ 0.03385234  0.02614976]\n",
      "After 15 epochs, Cost = -0.9935694580585444, Gradient Step = [ 0.02695368  0.0192967 ]\n",
      "After 16 epochs, Cost = -0.9932235303986765, Gradient Step = [ 0.02145877  0.01423534]\n",
      "After 17 epochs, Cost = -0.9929269322624016, Gradient Step = [ 0.01708315  0.0104994 ]\n",
      "After 18 epochs, Cost = -0.9926844341422604, Gradient Step = [ 0.01359931  0.00774283]\n",
      "After 19 epochs, Cost = -0.9924918222444212, Gradient Step = [ 0.01082574  0.00570939]\n",
      "After 20 epochs, Cost = -0.9923417066095376, Gradient Step = [ 0.00861775  0.00420965]\n",
      "After 21 epochs, Cost = -0.9922262300419604, Gradient Step = [ 0.00686004  0.00310367]\n",
      "After 22 epochs, Cost = -0.9921382266468595, Gradient Step = [ 0.00546082  0.00228816]\n",
      "After 23 epochs, Cost = -0.9920716210454941, Gradient Step = [ 0.00434698  0.00168686]\n",
      "After 24 epochs, Cost = -0.9920214719716689, Gradient Step = [ 0.00346033  0.00124353]\n",
      "After 25 epochs, Cost = -0.991983864207245, Gradient Step = [ 0.00275452  0.0009167 ]\n",
      "After 26 epochs, Cost = -0.9919557494607163, Gradient Step = [ 0.00219268  0.00067575]\n",
      "After 27 epochs, Cost = -0.9919347834827089, Gradient Step = [ 0.00174543  0.00049812]\n",
      "After 28 epochs, Cost = -0.9919191795486053, Gradient Step = [ 0.00138941  0.00036718]\n",
      "After 29 epochs, Cost = -0.9919075849543496, Gradient Step = [ 0.00110601  0.00027066]\n",
      "After 30 epochs, Cost = -0.9918989808087156, Gradient Step = [ 0.00088042  0.0001995 ]\n",
      "After 31 epochs, Cost = -0.9918926026987175, Gradient Step = [ 0.00070083  0.00014705]\n",
      "After 32 epochs, Cost = -0.9918878789240063, Gradient Step = [ 0.00055788  0.00010839]\n",
      "After 33 epochs, Cost = -0.9918843829803065, Gradient Step = [  4.44090905e-04   7.98964558e-05]\n"
     ]
    }
   ],
   "source": [
    "# Perform Vanilla Gradient Descent\n",
    "x_start = np.array([2.3, 3.9])\n",
    "lr = 0.95\n",
    "x_ = x_start\n",
    "tol = 1E-4\n",
    "grad_step = np.array([1,1])\n",
    "epoch = 1\n",
    "\n",
    "grad_x = np.empty((0,2))\n",
    "while(np.abs(grad_step[0]) > tol and np.abs(grad_step[1]) > tol ):\n",
    "    gradient = Grad_J(x_)\n",
    "    grad_step  = lr*gradient\n",
    "    x_ = x_ - grad_step\n",
    "    print ('After {} epochs, Cost = {}, Gradient Step = {}'.format(epoch, Comp_Dist(x_[0], x_[1]), grad_step))\n",
    "    grad_x = np.append(grad_x, x_.reshape(1,2), 0)\n",
    "    epoch += 1\n",
    "Plot(x,y,J,grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
